from typing import Any, Dict, List, Optional, Sequence
import base64
import os
import json
import re
from datetime import datetime, timedelta
from urllib.parse import quote

import google.auth
from google.auth.transport.requests import Request
from google.oauth2 import service_account
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# MCP
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("gsc-server")

# Path to your service account JSON or user credentials JSON
# First check if GSC_CREDENTIALS_PATH environment variable is set
# Then try looking in the script directory and current working directory as fallbacks
GSC_CREDENTIALS_PATH = os.environ.get("GSC_CREDENTIALS_PATH")
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
POSSIBLE_CREDENTIAL_PATHS = [
    GSC_CREDENTIALS_PATH,  # First try the environment variable if set
    os.path.join(SCRIPT_DIR, "service_account_credentials.json"),
    os.path.join(os.getcwd(), "service_account_credentials.json"),
    # Add any other potential paths here
]

# OAuth client secrets file path
OAUTH_CLIENT_SECRETS_FILE = os.environ.get("GSC_OAUTH_CLIENT_SECRETS_FILE")
if not OAUTH_CLIENT_SECRETS_FILE:
    OAUTH_CLIENT_SECRETS_FILE = os.path.join(SCRIPT_DIR, "client_secrets.json")

# Token file path for storing OAuth tokens
TOKEN_FILE = os.path.join(SCRIPT_DIR, "token.json")

# Environment variable to skip OAuth authentication
SKIP_OAUTH = os.environ.get("GSC_SKIP_OAUTH", "").lower() in ("true", "1", "yes")

SCOPES = ["https://www.googleapis.com/auth/webmasters"]

MAX_SEARCH_ANALYTICS_ROWS = 25000
DEFAULT_SEARCH_ANALYTICS_ROWS = 20
URL_IN_QUERY_PATTERN = re.compile(r"https?://[^\s>]+", re.IGNORECASE)


def _encode_result_id(payload: Dict[str, Any]) -> str:
    """Encode a payload dictionary into a compact, URL-safe identifier."""

    data = json.dumps(payload, sort_keys=True, separators=(",", ":")).encode("utf-8")
    return base64.urlsafe_b64encode(data).decode("utf-8").rstrip("=")


def _decode_result_id(result_id: str) -> Dict[str, Any]:
    """Decode the opaque identifier generated by ``_encode_result_id``."""

    padding = "=" * (-len(result_id) % 4)
    decoded = base64.urlsafe_b64decode(result_id + padding)
    return json.loads(decoded.decode("utf-8"))


def _property_label(site_url: str) -> str:
    """Return a human readable label for a Search Console property."""

    if site_url.startswith("sc-domain:"):
        return site_url[len("sc-domain:") :]
    return site_url.rstrip("/")


def _clean_extracted_url(raw_url: str) -> str:
    return raw_url.rstrip("),.;'\"")


def _property_matches_query(site_url: str, query: str) -> bool:
    """Check if a property string is mentioned in the query."""

    label = _property_label(site_url).lower()
    query_lower = query.lower()
    return site_url.lower() in query_lower or label in query_lower


def _console_url(site_url: str, view: str = "overview") -> str:
    """Build a Search Console UI URL for the given property and section."""

    resource_id = quote(site_url, safe="")
    base = "https://search.google.com/search-console"
    if view == "performance":
        return f"{base}/performance/search-analytics?resource_id={resource_id}"
    if view == "sitemaps":
        return f"{base}/sitemaps?resource_id={resource_id}"
    if view == "inspections":
        return f"{base}/inspections?resource_id={resource_id}"
    return f"{base}/about?resource_id={resource_id}"


def _list_properties_data(service: Optional[Any] = None) -> List[Dict[str, Any]]:
    """Return the list of Search Console properties with minimal metadata."""

    if service is None:
        service = get_gsc_service()
    site_list = service.sites().list().execute()
    return [
        {
            "siteUrl": site.get("siteUrl", ""),
            "permissionLevel": site.get("permissionLevel", "UNKNOWN"),
        }
        for site in site_list.get("siteEntry", [])
        if site.get("siteUrl")
    ]


def _detect_site_from_query(query: str, properties: Sequence[Dict[str, Any]]) -> Optional[str]:
    """Return the property URL that best matches the free-form query."""

    for prop in properties:
        site_url = prop.get("siteUrl")
        if site_url and _property_matches_query(site_url, query):
            return site_url

    if properties:
        return properties[0].get("siteUrl")
    return None


def _detect_site_for_page_url(
    page_url: str, properties: Sequence[Dict[str, Any]]
) -> Optional[str]:
    """Attempt to identify which property contains the inspected page."""

    normalized = page_url.lower()
    for prop in properties:
        site_url = prop.get("siteUrl")
        if not site_url:
            continue
        if site_url.startswith("sc-domain:"):
            domain = site_url[len("sc-domain:") :].lstrip(".").lower()
            if domain and domain in normalized:
                return site_url
        else:
            if normalized.startswith(site_url.lower()):
                return site_url

    return _detect_site_from_query(page_url, properties)


def _detect_days_from_query(query: str, default: int = 28) -> int:
    """Infer the desired lookback window from the query string."""

    query_lower = query.lower()
    match = re.search(r"(\d+)\s*(day|days)", query_lower)
    if match:
        value = int(match.group(1))
        return max(1, min(value, 180))

    match = re.search(r"(\d+)\s*(week|weeks)", query_lower)
    if match:
        value = int(match.group(1)) * 7
        return max(1, min(value, 180))

    match = re.search(r"(\d+)\s*(month|months)", query_lower)
    if match:
        value = int(match.group(1)) * 30
        return max(1, min(value, 180))

    return default


def _detect_dimensions_from_query(query: str) -> List[str]:
    """Infer which Search Analytics dimensions are relevant to the query."""

    query_lower = query.lower()
    if any(keyword in query_lower for keyword in ["page", "url", "landing"]):
        return ["page"]
    if "device" in query_lower:
        return ["device"]
    if "country" in query_lower or "region" in query_lower:
        return ["country"]
    if "date" in query_lower:
        return ["date"]
    return ["query"]


def _detect_search_type(query: str) -> str:
    query_lower = query.lower()
    if "image" in query_lower:
        return "IMAGE"
    if "video" in query_lower:
        return "VIDEO"
    if "news" in query_lower:
        return "NEWS"
    if "discover" in query_lower:
        return "DISCOVER"
    return "WEB"


def _detect_row_limit(query: str, default: int = DEFAULT_SEARCH_ANALYTICS_ROWS) -> int:
    match = re.search(r"top\s+(\d+)", query.lower())
    if match:
        return max(1, min(int(match.group(1)), MAX_SEARCH_ANALYTICS_ROWS))
    return default


def _classify_query_intent(query: str) -> str:
    """Classify the connector search intent based on keywords."""

    query_lower = query.lower()
    if URL_IN_QUERY_PATTERN.search(query):
        return "url_inspection"
    if any(keyword in query_lower for keyword in ["sitemap", "index file"]):
        return "sitemaps"
    if any(keyword in query_lower for keyword in ["property", "properties", "site list"]):
        return "properties"
    return "search_analytics"


def _get_search_analytics_data(
    site_url: str,
    *,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    days: int = 28,
    dimensions: Optional[Sequence[str]] = None,
    row_limit: int = DEFAULT_SEARCH_ANALYTICS_ROWS,
    start_row: int = 0,
    search_type: str = "WEB",
    order_by: str | None = "clicks",
    order_direction: str = "descending",
    filters: Optional[Sequence[Dict[str, Any]]] = None,
    service: Optional[Any] = None,
) -> Dict[str, Any]:
    """Retrieve Search Analytics data in a structured form."""

    if service is None:
        service = get_gsc_service()

    if not start_date or not end_date:
        end = datetime.utcnow().date()
        start = end - timedelta(days=days)
        start_date = start.strftime("%Y-%m-%d")
        end_date = end.strftime("%Y-%m-%d")

    dimension_list = list(dimensions or ["query"])
    bounded_row_limit = max(1, min(int(row_limit), MAX_SEARCH_ANALYTICS_ROWS))

    request_body: Dict[str, Any] = {
        "startDate": start_date,
        "endDate": end_date,
        "dimensions": dimension_list,
        "rowLimit": bounded_row_limit,
        "startRow": max(0, int(start_row)),
        "searchType": search_type.upper(),
    }

    if order_by:
        metric_map = {
            "clicks": "CLICK_COUNT",
            "impressions": "IMPRESSION_COUNT",
            "ctr": "CTR",
            "position": "POSITION",
        }
        metric = metric_map.get(order_by.lower())
        if metric:
            request_body["orderBy"] = [
                {
                    "metric": metric,
                    "direction": "ascending" if order_direction.lower() == "ascending" else "descending",
                }
            ]

    if filters:
        request_body["dimensionFilterGroups"] = [
            {
                "groupType": "and",
                "filters": list(filters),
            }
        ]

    response = (
        service.searchanalytics()
        .query(siteUrl=site_url, body=request_body)
        .execute()
    )

    return {
        "siteUrl": site_url,
        "startDate": start_date,
        "endDate": end_date,
        "dimensions": dimension_list,
        "rowLimit": bounded_row_limit,
        "startRow": request_body["startRow"],
        "searchType": request_body["searchType"],
        "filters": list(filters or []),
        "response": response,
        "rows": response.get("rows", []),
    }


def _summarize_search_analytics_title(
    data: Dict[str, Any],
    *,
    site_label: str,
    dimension_label: str,
) -> str:
    """Create a concise title describing the analytics snapshot."""

    days = (datetime.fromisoformat(data["endDate"]) - datetime.fromisoformat(data["startDate"])).days
    base_title = f"{site_label}: {dimension_label} performance (last {max(days, 1)} days)"
    rows = data.get("rows", [])
    if rows:
        top_row = rows[0]
        keys = top_row.get("keys", [])
        key_text = " / ".join(keys) if keys else "All"
        clicks = top_row.get("clicks", 0)
        impressions = top_row.get("impressions", 0)
        base_title += f" – top {dimension_label} {key_text} ({clicks} clicks, {impressions} impressions)"
    return base_title


def _format_search_analytics_text(data: Dict[str, Any]) -> str:
    """Convert structured analytics data into a readable report."""

    rows = data.get("rows", [])
    site_url = data.get("siteUrl", "")
    lines = [
        f"Search performance for {site_url}",
        f"Date range: {data.get('startDate')} to {data.get('endDate')}",
        f"Search type: {data.get('searchType', 'WEB')}",
    ]

    if data.get("filters"):
        filters_text = "; ".join(
            f"{flt.get('dimension')}={flt.get('expression')}" for flt in data["filters"]
        )
        lines.append(f"Filters: {filters_text}")

    dimensions = data.get("dimensions", [])
    if not rows:
        lines.append("\nNo search analytics data found for the requested parameters.")
        return "\n".join(lines)

    header = [dim.capitalize() for dim in dimensions] + [
        "Clicks",
        "Impressions",
        "CTR",
        "Position",
    ]
    lines.append("\n" + " | ".join(header))
    lines.append("-" * 80)

    for row in rows:
        dim_values = row.get("keys", [])
        line = list(dim_values)
        line.append(str(row.get("clicks", 0)))
        line.append(str(row.get("impressions", 0)))
        line.append(f"{row.get('ctr', 0) * 100:.2f}%")
        line.append(f"{row.get('position', 0):.1f}")
        lines.append(" | ".join(line))

    return "\n".join(lines)


def _build_filters_for_keys(
    dimensions: Sequence[str], keys: Sequence[str]
) -> List[Dict[str, Any]]:
    filters: List[Dict[str, Any]] = []
    for dimension, key in zip(dimensions, keys):
        filters.append(
            {
                "dimension": dimension,
                "operator": "equals",
                "expression": key,
            }
        )
    return filters


def _format_search_analytics_row_text(
    data: Dict[str, Any], keys: Sequence[str]
) -> str:
    site_url = data.get("siteUrl", "")
    lines = [
        f"Search performance detail for {site_url}",
        f"Date range: {data.get('startDate')} to {data.get('endDate')}",
        f"Search type: {data.get('searchType', 'WEB')}",
    ]

    label = " / ".join(keys) if keys else "All"
    lines.append(f"Dimension values: {label}")

    row = data.get("rows", [{}])[0] if data.get("rows") else {}
    lines.append("")
    lines.append(f"Clicks: {row.get('clicks', 0)}")
    lines.append(f"Impressions: {row.get('impressions', 0)}")
    lines.append(f"CTR: {row.get('ctr', 0) * 100:.2f}%")
    lines.append(f"Average position: {row.get('position', 0):.1f}")

    return "\n".join(lines)


def _get_sitemaps_data(site_url: str, service: Optional[Any] = None) -> Dict[str, Any]:
    """Return structured sitemap information for a property."""

    if service is None:
        service = get_gsc_service()

    response = service.sitemaps().list(siteUrl=site_url).execute()
    entries = []
    for item in response.get("sitemap", []):
        contents = item.get("contents", []) or []
        submitted = None
        indexed = None
        for content in contents:
            if content.get("type") == "web":
                submitted = content.get("submitted")
                indexed = content.get("indexed")
                break

        entries.append(
            {
                "path": item.get("path"),
                "lastSubmitted": item.get("lastSubmitted"),
                "lastDownloaded": item.get("lastDownloaded"),
                "isSitemapsIndex": item.get("isSitemapsIndex", False),
                "isPending": item.get("isPending", False),
                "warnings": item.get("warnings", 0),
                "errors": item.get("errors", 0),
                "submittedUrls": submitted,
                "indexedUrls": indexed,
            }
        )

    return {
        "siteUrl": site_url,
        "sitemaps": entries,
        "response": response,
        "fetchedAt": datetime.utcnow().isoformat() + "Z",
    }


def _summarize_sitemap_entry(entry: Dict[str, Any]) -> str:
    errors = entry.get("errors", 0) or 0
    warnings = entry.get("warnings", 0) or 0
    indexed = entry.get("indexedUrls")
    if errors:
        status = f"{errors} errors"
    elif warnings:
        status = f"{warnings} warnings"
    elif entry.get("isPending"):
        status = "Pending processing"
    else:
        status = "Processed"

    if indexed is not None:
        return f"{status}, {indexed} indexed URLs"
    return status


def _format_sitemaps_text(data: Dict[str, Any], *, target_path: Optional[str] = None) -> str:
    entries = data.get("sitemaps", [])
    site_url = data.get("siteUrl", "")
    lines = [f"Sitemaps for {site_url}"]

    if not entries:
        lines.append("No sitemaps found for this property.")
        return "\n".join(lines)

    lines.append("Path | Status | Indexed URLs | Last Downloaded")
    lines.append("-" * 80)

    for entry in entries:
        if target_path and entry.get("path") != target_path:
            continue

        status = _summarize_sitemap_entry(entry)
        lines.append(
            f"{entry.get('path')} | {status} | {entry.get('indexedUrls', 'N/A')} | {entry.get('lastDownloaded', 'Never')}"
        )

    if target_path and all(entry.get("path") != target_path for entry in entries):
        lines.append(f"No sitemap found with path {target_path}.")

    return "\n".join(lines)


def _find_matching_sitemap(
    query: str, entries: Sequence[Dict[str, Any]]
) -> Optional[Dict[str, Any]]:
    query_lower = query.lower()
    for entry in entries:
        path = entry.get("path")
        if not path:
            continue
        path_lower = path.lower()
        if path_lower in query_lower:
            return entry
        filename = path_lower.rsplit("/", 1)[-1]
        if filename and filename in query_lower:
            return entry
    return None


def _get_site_details_data(site_url: str, service: Optional[Any] = None) -> Dict[str, Any]:
    if service is None:
        service = get_gsc_service()
    response = service.sites().get(siteUrl=site_url).execute()
    return {"siteUrl": site_url, "details": response}


def _format_site_details_text(data: Dict[str, Any]) -> str:
    site_url = data.get("siteUrl", "")
    details = data.get("details", {})
    lines = [f"Site details for {site_url}"]
    permission = details.get("permissionLevel", "Unknown")
    lines.append(f"Permission level: {permission}")

    verification = details.get("siteVerificationInfo", {})
    if verification:
        lines.append(f"Verification state: {verification.get('verificationState', 'Unknown')}")
        if verification.get("verifiedUser"):
            lines.append(f"Verified by: {verification['verifiedUser']}")
        if verification.get("verificationMethod"):
            lines.append(f"Verification method: {verification['verificationMethod']}")

    ownership = details.get("ownershipInfo", {})
    if ownership:
        lines.append("Ownership information:")
        owner = ownership.get("owner")
        if owner:
            lines.append(f"- Owner: {owner}")
        if ownership.get("verificationMethod"):
            lines.append(f"- Method: {ownership['verificationMethod']}")

    return "\n".join(lines)


def _get_url_inspection_data(
    site_url: str,
    page_url: str,
    service: Optional[Any] = None,
) -> Dict[str, Any]:
    if service is None:
        service = get_gsc_service()

    request = {
        "inspectionUrl": page_url,
        "siteUrl": site_url,
    }
    response = service.urlInspection().index().inspect(body=request).execute()
    inspection = response.get("inspectionResult", {})

    return {
        "siteUrl": site_url,
        "inspectionUrl": page_url,
        "response": response,
        "indexStatus": inspection.get("indexStatusResult", {}),
        "mobileUsability": inspection.get("mobileUsabilityResult", {}),
        "richResults": inspection.get("richResultsResult", {}),
    }


def _format_url_inspection_text(data: Dict[str, Any]) -> str:
    index_status = data.get("indexStatus", {})
    mobile = data.get("mobileUsability", {})
    rich = data.get("richResults", {})

    lines = [
        f"URL inspection for {data.get('inspectionUrl')}",
        f"Property: {data.get('siteUrl')}",
    ]

    verdict = index_status.get("verdict", "UNKNOWN")
    coverage = index_status.get("coverageState", "Unknown")
    lines.append(f"Index status: {verdict} – {coverage}")

    if index_status.get("lastCrawlTime"):
        lines.append(f"Last crawl: {index_status['lastCrawlTime']}")
    if index_status.get("pageFetchState"):
        lines.append(f"Fetch state: {index_status['pageFetchState']}")
    if index_status.get("robotsTxtState"):
        lines.append(f"robots.txt: {index_status['robotsTxtState']}")

    if mobile:
        lines.append(
            f"Mobile usability: {mobile.get('verdict', 'UNKNOWN')}"
        )

    if rich:
        verdict = rich.get("verdict")
        lines.append(f"Rich results: {verdict}")
        detected = rich.get("detectedItems", [])
        if detected:
            lines.append("Detected rich result types:")
            for item in detected[:3]:
                lines.append(f"- {item.get('richResultType', 'Unknown')}")
            if len(detected) > 3:
                lines.append(f"- ... and {len(detected) - 3} more")

    return "\n".join(lines)

def get_gsc_service():
    """
    Returns an authorized Search Console service object.
    First tries OAuth authentication, then falls back to service account.
    """
    # Try OAuth authentication first if not skipped
    if not SKIP_OAUTH:
        try:
            return get_gsc_service_oauth()
        except Exception as e:
            # If OAuth fails, try service account
            pass
    
    # Try service account authentication
    for cred_path in POSSIBLE_CREDENTIAL_PATHS:
        if cred_path and os.path.exists(cred_path):
            try:
                creds = service_account.Credentials.from_service_account_file(
                    cred_path, scopes=SCOPES
                )
                return build("searchconsole", "v1", credentials=creds)
            except Exception as e:
                continue  # Try the next path if this one fails
    
    # If we get here, none of the authentication methods worked
@@ -90,61 +651,51 @@ def get_gsc_service_oauth():
        else:
            # Check if client secrets file exists
            if not os.path.exists(OAUTH_CLIENT_SECRETS_FILE):
                raise FileNotFoundError(
                    f"OAuth client secrets file not found. Please place a client_secrets.json file in the script directory "
                    f"or set the GSC_OAUTH_CLIENT_SECRETS_FILE environment variable."
                )
            
            # Start OAuth flow
            flow = InstalledAppFlow.from_client_secrets_file(OAUTH_CLIENT_SECRETS_FILE, SCOPES)
            creds = flow.run_local_server(port=0)
            
            # Save the credentials for future use
            with open(TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())
    
    # Build and return the service
    return build("searchconsole", "v1", credentials=creds)

@mcp.tool()
async def list_properties() -> str:
    """
    Retrieves and returns the user's Search Console properties.
    """
    try:
        sites = _list_properties_data()

        if not sites:
            return "No Search Console properties found."

        # Format the results for easy reading
        lines = []
        for site in sites:
            site_url = site.get("siteUrl", "Unknown")
            permission = site.get("permissionLevel", "Unknown permission")
            lines.append(f"- {site_url} ({permission})")

        return "\n".join(lines)
    except FileNotFoundError as e:
        return (
            "Error: Service account credentials file not found.\n\n"
            "To access Google Search Console, please:\n"
            "1. Create a service account in Google Cloud Console\n"
            "2. Download the JSON credentials file\n"
            "3. Save it as 'service_account_credentials.json' in the same directory as this script\n"
            "4. Share your GSC properties with the service account email"
        )
    except Exception as e:
        return f"Error retrieving properties: {str(e)}"

@mcp.tool()
@@ -242,101 +793,65 @@ async def delete_site(site_url: str) -> str:
        elif error_code == 401:
            return f"Error: Unauthorized. Please check your credentials."
        elif error_code == 429:
            return f"Error: Too many requests. Please try again later."
        elif error_code == 500:
            return f"Error: Internal server error from Google Search Console API. Please try again later."
        elif error_code == 503:
            return f"Error: Service unavailable. Google Search Console API is currently down. Please try again later."
        else:
            return f"Error removing site (HTTP {error_code}): {error_message}"
    except Exception as e:
        return f"Error removing site: {str(e)}"

@mcp.tool()
async def get_search_analytics(site_url: str, days: int = 28, dimensions: str = "query") -> str:
    """
    Get search analytics data for a specific property.
    
    Args:
        site_url: The URL of the site in Search Console (must be exact match)
        days: Number of days to look back (default: 28)
        dimensions: Dimensions to group by (default: query). Options: query, page, device, country, date
                   You can provide multiple dimensions separated by comma (e.g., "query,page")
    """
    try:
        dimension_list = [d.strip() for d in dimensions.split(",") if d.strip()]
        if not dimension_list:
            dimension_list = ["query"]

        data = _get_search_analytics_data(
            site_url,
            days=days,
            dimensions=dimension_list,
            row_limit=DEFAULT_SEARCH_ANALYTICS_ROWS,
        )

        if not data.get("rows"):
            return f"No search analytics data found for {site_url} in the last {days} days."
        return _format_search_analytics_text(data)

    except Exception as e:
        return f"Error retrieving search analytics: {str(e)}"

@mcp.tool()
async def get_site_details(site_url: str) -> str:
    """
    Get detailed information about a specific Search Console property.
    
    Args:
        site_url: The URL of the site in Search Console (must be exact match)
    """
    try:
        service = get_gsc_service()
        
        # Get site details
        site_info = service.sites().get(siteUrl=site_url).execute()
        
        # Format the results
        result_lines = [f"Site details for {site_url}:"]
        result_lines.append("-" * 50)
        
        # Add basic info
        result_lines.append(f"Permission level: {site_info.get('permissionLevel', 'Unknown')}")
        
        # Add verification info if available
@@ -350,217 +865,74 @@ async def get_site_details(site_url: str) -> str:
            if "verificationMethod" in verify_info:
                result_lines.append(f"Verification method: {verify_info['verificationMethod']}")
        
        # Add ownership info if available
        if "ownershipInfo" in site_info:
            owner_info = site_info["ownershipInfo"]
            result_lines.append("\nOwnership Information:")
            result_lines.append(f"Owner: {owner_info.get('owner', 'Unknown')}")
            
            if "verificationMethod" in owner_info:
                result_lines.append(f"Ownership verification: {owner_info['verificationMethod']}")
        
        return "\n".join(result_lines)
    except Exception as e:
        return f"Error retrieving site details: {str(e)}"

@mcp.tool()
async def get_sitemaps(site_url: str) -> str:
    """
    List all sitemaps for a specific Search Console property.
    
    Args:
        site_url: The URL of the site in Search Console (must be exact match)
    """
    try:
        data = _get_sitemaps_data(site_url)
        if not data.get("sitemaps"):
            return f"No sitemaps found for {site_url}."
        return _format_sitemaps_text(data)
    except Exception as e:
        return f"Error retrieving sitemaps: {str(e)}"

@mcp.tool()
async def inspect_url_enhanced(site_url: str, page_url: str) -> str:
    """
    Enhanced URL inspection to check indexing status and rich results in Google.
    Args:
        site_url: The URL of the site in Search Console (must be exact match, for domain properties use format: sc-domain:example.com)
        page_url: The specific URL to inspect
    """
    try:
        data = _get_url_inspection_data(site_url, page_url)
        response = data.get("response", {})
        if not response or "inspectionResult" not in response:
            return f"No inspection data found for {page_url}."
        return _format_url_inspection_text(data)


    except Exception as e:
        return f"Error inspecting URL: {str(e)}"

@mcp.tool()
async def batch_url_inspection(site_url: str, urls: str) -> str:
    """
    Inspect multiple URLs in batch (within API limits).
    
    Args:
        site_url: The URL of the site in Search Console (must be exact match, for domain properties use format: sc-domain:example.com)
        urls: List of URLs to inspect, one per line
    """
    try:
        service = get_gsc_service()
        
        # Parse URLs
        url_list = [url.strip() for url in urls.split('\n') if url.strip()]
        
        if not url_list:
            return "No URLs provided for inspection."
        
        if len(url_list) > 10:
            return f"Too many URLs provided ({len(url_list)}). Please limit to 10 URLs per batch to avoid API quota issues."
        
        # Process each URL
@@ -1385,50 +1757,360 @@ async def manage_sitemaps(site_url: str, action: str, sitemap_url: str = None, s
    """
    try:
        # Validate inputs
        action = action.lower().strip()
        valid_actions = ["list", "details", "submit", "delete"]
        
        if action not in valid_actions:
            return f"Invalid action: {action}. Please use one of: {', '.join(valid_actions)}"
        
        if action in ["details", "submit", "delete"] and not sitemap_url:
            return f"The {action} action requires a sitemap_url parameter."
        
        # Perform the requested action
        if action == "list":
            return await list_sitemaps_enhanced(site_url, sitemap_index)
        elif action == "details":
            return await get_sitemap_details(site_url, sitemap_url)
        elif action == "submit":
            return await submit_sitemap(site_url, sitemap_url)
        elif action == "delete":
            return await delete_sitemap(site_url, sitemap_url)
    
    except Exception as e:
        return f"Error managing sitemaps: {str(e)}"

@mcp.tool(name="search")
async def connector_search(query: str) -> Dict[str, Any]:
    """Search tool that exposes GSC data for OpenAI connectors."""

    try:
        service = get_gsc_service()
        properties = _list_properties_data(service)
    except FileNotFoundError as exc:
        raise RuntimeError(str(exc)) from exc
    except Exception as exc:
        raise RuntimeError(f"Failed to load Search Console properties: {exc}") from exc

    if not properties:
        return {"results": []}

    site_url = _detect_site_from_query(query, properties)
    if not site_url:
        return {"results": []}

    intent = _classify_query_intent(query)
    site_label = _property_label(site_url)
    results: List[Dict[str, str]] = []

    if intent == "properties":
        for prop in properties:
            site = prop.get("siteUrl")
            if not site:
                continue
            payload = {"type": "property", "siteUrl": site}
            results.append(
                {
                    "id": _encode_result_id(payload),
                    "title": f"Search Console property {site}",
                    "url": _console_url(site, "overview"),
                }
            )
        return {"results": results}

    if intent == "sitemaps":
        sitemap_data = _get_sitemaps_data(site_url, service=service)
        entries = sitemap_data.get("sitemaps", [])
        matched = _find_matching_sitemap(query, entries)

        if matched:
            payload = {
                "type": "sitemap",
                "siteUrl": site_url,
                "path": matched.get("path"),
            }
            results.append(
                {
                    "id": _encode_result_id(payload),
                    "title": f"Sitemap {matched.get('path')} – {_summarize_sitemap_entry(matched)}",
                    "url": _console_url(site_url, "sitemaps"),
                }
            )
        else:
            overview_payload = {"type": "sitemaps_overview", "siteUrl": site_url}
            results.append(
                {
                    "id": _encode_result_id(overview_payload),
                    "title": f"Sitemaps overview for {site_label}",
                    "url": _console_url(site_url, "sitemaps"),
                }
            )
            for entry in entries[:3]:
                payload = {
                    "type": "sitemap",
                    "siteUrl": site_url,
                    "path": entry.get("path"),
                }
                results.append(
                    {
                        "id": _encode_result_id(payload),
                        "title": f"Sitemap {entry.get('path')} – {_summarize_sitemap_entry(entry)}",
                        "url": _console_url(site_url, "sitemaps"),
                    }
                )

        return {"results": results}

    if intent == "url_inspection":
        match = URL_IN_QUERY_PATTERN.search(query)
        if not match:
            return {"results": []}
        page_url = _clean_extracted_url(match.group(0))
        site_for_page = _detect_site_for_page_url(page_url, properties) or site_url

        title = f"Index inspection for {page_url}"
        try:
            inspection = _get_url_inspection_data(site_for_page, page_url, service=service)
            index_status = inspection.get("indexStatus", {})
            verdict = index_status.get("verdict")
            coverage = index_status.get("coverageState")
            summary_bits = [bit for bit in [verdict, coverage] if bit]
            if summary_bits:
                title += " – " + " / ".join(summary_bits)
        except Exception:
            pass

        payload = {
            "type": "url_inspection",
            "siteUrl": site_for_page,
            "pageUrl": page_url,
        }
        results.append(
            {
                "id": _encode_result_id(payload),
                "title": title,
                "url": _console_url(site_for_page, "inspections"),
            }
        )
        return {"results": results}

    dimensions = _detect_dimensions_from_query(query)
    days = _detect_days_from_query(query)
    row_limit = _detect_row_limit(query)
    search_type = _detect_search_type(query)

    analytics_data = _get_search_analytics_data(
        site_url,
        days=days,
        dimensions=dimensions,
        row_limit=row_limit,
        search_type=search_type,
        service=service,
    )

    dimension_label = ", ".join(dimensions) if dimensions else "overall"

    overview_payload = {
        "type": "search_analytics",
        "siteUrl": site_url,
        "dimensions": dimensions,
        "days": days,
        "rowLimit": row_limit,
        "searchType": search_type,
        "startDate": analytics_data.get("startDate"),
        "endDate": analytics_data.get("endDate"),
        "startRow": analytics_data.get("startRow", 0),
    }

    results.append(
        {
            "id": _encode_result_id(overview_payload),
            "title": _summarize_search_analytics_title(
                analytics_data,
                site_label=site_label,
                dimension_label=dimension_label or "overall",
            ),
            "url": _console_url(site_url, "performance"),
        }
    )

    rows = analytics_data.get("rows", [])
    for row in rows[:3]:
        keys = row.get("keys", [])
        filters = _build_filters_for_keys(dimensions, keys)
        key_label = " / ".join(keys) if keys else "All"
        metrics_summary = f"{row.get('clicks', 0)} clicks, {row.get('impressions', 0)} impressions"
        payload = {
            "type": "search_analytics_row",
            "siteUrl": site_url,
            "dimensions": dimensions,
            "keys": keys,
            "days": days,
            "searchType": search_type,
            "rowLimit": 1,
            "startDate": analytics_data.get("startDate"),
            "endDate": analytics_data.get("endDate"),
            "filters": filters,
        }
        results.append(
            {
                "id": _encode_result_id(payload),
                "title": f"{dimension_label.title()} {key_label} – {metrics_summary} (last {days} days)",
                "url": _console_url(site_url, "performance"),
            }
        )

    return {"results": results}


@mcp.tool(name="fetch")
async def connector_fetch(document_id: str) -> Dict[str, Any]:
    """Fetch tool that returns structured documents for OpenAI connectors."""

    try:
        payload = _decode_result_id(document_id)
    except Exception as exc:
        raise RuntimeError("Invalid document identifier") from exc

    doc_type = payload.get("type")

    try:
        if doc_type == "property":
            site_url = payload["siteUrl"]
            data = _get_site_details_data(site_url)
            text = _format_site_details_text(data)
            title = f"Search Console property {site_url}"
            url = _console_url(site_url, "overview")
            metadata = {"type": doc_type, "siteUrl": site_url, "details": data.get("details")}

        elif doc_type == "sitemaps_overview":
            site_url = payload["siteUrl"]
            data = _get_sitemaps_data(site_url)
            text = _format_sitemaps_text(data)
            title = f"Sitemaps overview for {_property_label(site_url)}"
            url = _console_url(site_url, "sitemaps")
            metadata = {"type": doc_type, "siteUrl": site_url, "sitemaps": data.get("sitemaps", [])}

        elif doc_type == "sitemap":
            site_url = payload["siteUrl"]
            data = _get_sitemaps_data(site_url)
            path = payload.get("path")
            text = _format_sitemaps_text(data, target_path=path)
            title = f"Sitemap {path} for {_property_label(site_url)}"
            url = _console_url(site_url, "sitemaps")
            metadata = {
                "type": doc_type,
                "siteUrl": site_url,
                "path": path,
                "sitemaps": data.get("sitemaps", []),
            }

        elif doc_type == "search_analytics":
            site_url = payload["siteUrl"]
            data = _get_search_analytics_data(
                site_url,
                start_date=payload.get("startDate"),
                end_date=payload.get("endDate"),
                days=payload.get("days", 28),
                dimensions=payload.get("dimensions"),
                row_limit=payload.get("rowLimit", DEFAULT_SEARCH_ANALYTICS_ROWS),
                search_type=payload.get("searchType", "WEB"),
                start_row=payload.get("startRow", 0),
            )
            text = _format_search_analytics_text(data)
            title = _summarize_search_analytics_title(
                data,
                site_label=_property_label(site_url),
                dimension_label=", ".join(data.get("dimensions", [])) or "overall",
            )
            url = _console_url(site_url, "performance")
            metadata = {
                "type": doc_type,
                "siteUrl": site_url,
                "parameters": payload,
                "rows": data.get("rows", []),
            }

        elif doc_type == "search_analytics_row":
            site_url = payload["siteUrl"]
            dimensions = payload.get("dimensions", [])
            keys = payload.get("keys", [])
            filters = payload.get("filters") or _build_filters_for_keys(dimensions, keys)
            data = _get_search_analytics_data(
                site_url,
                start_date=payload.get("startDate"),
                end_date=payload.get("endDate"),
                days=payload.get("days", 28),
                dimensions=dimensions,
                row_limit=payload.get("rowLimit", 1),
                search_type=payload.get("searchType", "WEB"),
                filters=filters,
            )
            text = _format_search_analytics_row_text(data, keys)
            title = f"Search analytics for {' / '.join(keys) if keys else 'All'}"
            url = _console_url(site_url, "performance")
            metadata = {
                "type": doc_type,
                "siteUrl": site_url,
                "dimensions": dimensions,
                "keys": keys,
                "rows": data.get("rows", []),
            }

        elif doc_type == "url_inspection":
            site_url = payload["siteUrl"]
            page_url = payload["pageUrl"]
            data = _get_url_inspection_data(site_url, page_url)
            response = data.get("response", {})
            if not response or "inspectionResult" not in response:
                text = f"No inspection data found for {page_url}."
            else:
                text = _format_url_inspection_text(data)
            title = f"Index inspection for {page_url}"
            url = _console_url(site_url, "inspections")
            metadata = {
                "type": doc_type,
                "siteUrl": site_url,
                "pageUrl": page_url,
                "inspection": data,
            }

        else:
            raise RuntimeError(f"Unknown document type: {doc_type}")

    except Exception as exc:
        raise RuntimeError(f"Failed to fetch document {document_id}: {exc}") from exc

    return {
        "id": document_id,
        "title": title,
        "text": text,
        "url": url,
        "metadata": metadata,
    }


@mcp.tool()
async def get_creator_info() -> str:
    """
    Provides information about Amin Foroutan, the creator of the MCP-GSC tool.
    """
    creator_info = """
# About the Creator: Amin Foroutan

Amin Foroutan is an SEO consultant with over a decade of experience, specializing in technical SEO, Python-driven tools, and data analysis for SEO performance.

## Connect with Amin:

- **LinkedIn**: [Amin Foroutan](https://www.linkedin.com/in/ma-foroutan/)
- **Personal Website**: [aminforoutan.com](https://aminforoutan.com/)
- **YouTube**: [Amin Forout](https://www.youtube.com/channel/UCW7tPXg-rWdH4YzLrcAdBIw)
- **X (Twitter)**: [@aminfseo](https://x.com/aminfseo)

## Notable Projects:

Amin has created several popular SEO tools including:
- Advanced GSC Visualizer (6.4K+ users)
- SEO Render Insight Tool (3.5K+ users)
- Google AI Overview Impact Analysis (1.2K+ users)
- Google AI Overview Citation Analysis (900+ users)
- SEMRush Enhancer (570+ users)